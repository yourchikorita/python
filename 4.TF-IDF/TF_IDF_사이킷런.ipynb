{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF-IDF_사이킷런.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64puMHKlmZwC",
        "colab_type": "text"
      },
      "source": [
        "# TF-IDF(Term Frequency-Inverse Document Frequency)\n",
        "이번 챕터에서는 DTM 내에 있는 각 단어에 대한 중요도를 계산할 수 있는 TF-IDF 가중치에 대해서 알아보도록 하겠습니다. TF-IDF를 사용하면, 기존의 DTM을 사용하는 것보다 보다 정확하게 문서들을 비교할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OYQb15tmhU2",
        "colab_type": "text"
      },
      "source": [
        "TF-IDF(Term Frequency-Inverse Document Frequency)는 단어의 빈도와 역 문서 빈도(문서의 빈도에 특정 식을 취함)를 사용하여 DTM 내의 각 단어들마다 중요한 정도를 가중치로 주는 방법입니다. 사용 방법은 우선 DTM을 만든 후, TF-IDF 가중치를 부여합니다.\n",
        "\n",
        "TF-IDF는 주로 문서의 유사도를 구하는 작업, 검색 시스템에서 검색 결과의 중요도를 정하는 작업, 문서 내에서 특정 단어의 중요도를 구하는 작업 등에 쓰일 수 있습니다.\n",
        "\n",
        "TF-IDF는 TF와 IDF를 곱한 값을 의미하는데 이를 식으로 표현해보겠습니다. 문서를 d, 단어를 t, 문서의 총 개수를 n이라고 표현할 때 TF, DF, IDF는 각각 다음과 같이 정의할 수 있습니다.\n",
        "\n",
        "(1) tf(d,t) : 특정 문서 d에서의 특정 단어 t의 등장 횟수.\n",
        "생소한 글자때문에 어려워보일 수 있지만, 잘 생각해보면 TF는 이미 앞에서 구한 적이 있습니다. TF는 앞에서 배운 DTM의 예제에서 각 단어들이 가진 값들입니다. DTM이 각 문서에서의 각 단어의 등장 빈도를 나타내는 값이었기 때문입니다.\n",
        "\n",
        "(2) df(t) : 특정 단어 t가 등장한 문서의 수.\n",
        "여기서 특정 단어가 각 문서, 또는 문서들에서 몇 번 등장했는지는 관심가지지 않으며 오직 특정 단어 t가 등장한 문서의 수에만 관심을 가집니다. 앞서 배운 DTM에서 바나나는 문서2와 문서3에서 등장했습니다. 이 경우, 바나나의 df는 2입니다. 문서3에서 바나나가 두 번 등장했지만, 그것은 중요한 게 아닙니다. 심지어 바나나란 단어가 문서2에서 100번 등장했고, 문서3에서 200번 등장했다고 하더라도 바나나의 df는 2가 됩니다.\n",
        "\n",
        "\n",
        "**TF-IDF는 모든 문서에서 자주 등장하는 단어는 중요도가 낮다고 판단하며, 특정 문서에서만 자주 등장하는 단어는 중요도가 높다고 판단합니다. TF-IDF 값이 낮으면 중요도가 낮은 것이며, TF-IDF 값이 크면 중요도가 큰 것입니다. 즉, the나 a와 같이 불용어의 경우에는 모든 문서에 자주 등장하기 마련이기 때문에 자연스럽게 불용어의 TF-IDF의 값은 다른 단어의 TF-IDF에 비해서 낮아지게 됩니다.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4tMEyL2nAfR",
        "colab_type": "text"
      },
      "source": [
        "## 사이킷런을 이용한 DTM과 TF-IDF 실습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spWFYVQhmeFw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "34717fd1-a553-4037-e7ec-c8035a555101"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\n",
        "    'you know I want your love',\n",
        "    'I like you',\n",
        "    'what should I do ',    \n",
        "]\n",
        "vector = CountVectorizer()\n",
        "print(vector.fit_transform(corpus).toarray()) # 코퍼스로부터 각 단어의 빈도 수를 기록한다.\n",
        "print(vector.vocabulary_) # 각 단어의 인덱스가 어떻게 부여되었는지를 보여준다."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 0 1 0 1 0 1 1]\n",
            " [0 0 1 0 0 0 0 1 0]\n",
            " [1 0 0 0 1 0 1 0 0]]\n",
            "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haLrhRvlo99X",
        "colab_type": "text"
      },
      "source": [
        "DTM이 완성되었습니다. DTM에서 각 단어의 인덱스가 어떻게 부여되었는지를 확인하기 위해, 인덱스를 확인해보았습니다. 첫번째 열의 경우에는 0의 인덱스를 가진 do입니다. do는 세번째 문서에만 등장했기 때문에, 세번째 행에서만 1의 값을 가집니다. 두번째 열의 경우에는 1의 인덱스를 가진 know입니다. know는 첫번째 문서에만 등장했기 때문에 첫번째 행에서만 1의 값을 가집니다.\n",
        "\n",
        "사이킷런은 TF-IDF를 자동 계산해주는 TfidfVectorizer를 제공합니다. 향후 실습을 하다가 혼란이 생기지 않도록 언급하자면, 사이킷런의 TF-IDF는 위에서 배웠던 보편적인 TF-IDF 식에서 좀 더 조정된 다른 식을 사용합니다. 하지만 크게 다른 식은 아니며(IDF 계산 시 분자에다가도 1을 더해주며, TF-IDF에 L2 정규화라는 방법으로 값을 조정하는 등의 차이), 여전히 TF-IDF가 가진 의도를 그대로 갖고 있으므로 사이킷런의 TF-IDF를 그대로 사용하셔도 좋습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELkd9XOLotFy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "255475a8-6703-4dde-d897-538bfc4a8be6"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus = [\n",
        "    'you know I want your love',\n",
        "    'I like you',\n",
        "    'what should I do ',    \n",
        "]\n",
        "tfidfv = TfidfVectorizer().fit(corpus)\n",
        "print(tfidfv.transform(corpus).toarray())\n",
        "print(tfidfv.vocabulary_)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.46735098 0.         0.46735098 0.         0.46735098\n",
            "  0.         0.35543247 0.46735098]\n",
            " [0.         0.         0.79596054 0.         0.         0.\n",
            "  0.         0.60534851 0.        ]\n",
            " [0.57735027 0.         0.         0.         0.57735027 0.\n",
            "  0.57735027 0.         0.        ]]\n",
            "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSgqJ_UKpMpv",
        "colab_type": "text"
      },
      "source": [
        "문서들 간의 유사도를 구하기 위한 재료 손질하는 방법을 배운 셈"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cnn93yxopE1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}